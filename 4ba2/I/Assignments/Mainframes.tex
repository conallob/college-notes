% $Id$

\documentclass[a4paper,12pt]{article}

\setlength{\parindent}{0mm}
\setlength{\parskip}{7.5mm}

\begin{document}

\bibliographystyle{ieeetr}

\title{4BA2 \\ Assignment 2}

\author{Brian Brazil (02017610) brazilb@tcd.ie \\  Morgan McEvoy
(02460955) mcevoymj@tcd.ie \\ Conall O'Brien (01734351)
conallob@maths.tcd.ie \\ Eamon Phelan (02406390) phelanec@tcd.ie}

\maketitle

\section{What is a Mainframe?}

\cite[Mainframes (often colloquially referred to as big iron) are large and 
``expensive'' computers used mainly by government institutions and 
large companies for mission critical applications, typically bulk data 
processing such as censuses, industry/consumer statistics, ERP, and 
financial transaction processing]{m1}


Nowadays IBM, Unisys, Fujitsu and Bull all remain selling mainframes,
with \cite[IBM mainframes dominating the market at over 90\%]{m1}.
Between the 1950s and 1970s, mainframes were also manufactured by 
Sperry and Burroughs (now owned by Unisys), Siemens and Amdahl (now 
owned by Fujitsu), Hitachi, Control Data Corporation, General Electric, 
Honeywell, NCR, RCA, UNIVAC, Telefunken and ICL. \cite [ IBM's 
dominance grew out of their development of the 360 series mainframes; 
this basic architecture has continued to evolve into their current 
zSeries/z9 mainframe, which is arguably the only mainframe architecture 
still extant that dates from this early period.]{m1}

\section{Reliability}

Historically one of the key features of mainframe has been its reliability; a
mainframe installation is typically designed to run continuously for years at a
time without interruption, capable of being repaired on-the-fly as individual
components fault without any interruption to services. This is provided for by
a variety of design elements, primarily an emphasis on redundancy; systems
possessed sufficient independent resource units that processes could run
happily through the aforementioned in situ repairs, while some modern systems
are exploring 'lock-step' processing where instructions are executed twice on
separate processors to aid in error-checking. So important to the mainframe
customer is this reliability that many or most vendors offer off-site
redundancy to guarantee that in the event of a failure their own hardware will
take over the task until repairs are complete - again, often without
interruption to services.

Mainframes have held their own against other models of supercomputing because
their typical applications, requiring reliability and consistency of operation,
have mandated design choices that contribute considerably to their robustness.
The integrated design means that their smaller number of processors is
compensated for by the absence of inter-processor and inter-machine
communication overhead; a fixed, streamlined architecture eliminates many
opportunities for breakdown. Further, the typical mainframe's focus on mixed
workloads and optimised I/O over multiprocessing and computational complexity
makes it well-suited for certain tasks which simply cannot be easily
restructured for supercomputers - it will not help to add machines to a cluster
if database access is your bottleneck, and having many processors to hand is
little help if the vital computations are simple, such as in financial
transactions (a mainframe niche). The unified architecture allows for far
better security than can be obtained via networked or grid computing.
Mainframes and their characteristic applications have thus grown into each
other. In addition, reliable systems such as Linux port well to mainframe
architectures where multiprocessor systems face considerable OS-related
headaches.

The reliability of the mainframe platform extends to consistency of operation
as well. Given the long uptimes and high setup costs of mainframe systems, they
are natural homes of 'legacy code'; once a program can keep doing its job in a
mainframe environment it is often a losing proposition to upgrade it. For this
reason backwards compatibility has long been a feature of mainframe design and
modern IBM systems in particular can still run code from 1960's systems without
modification. The operating system-level handling of parallelism insulates such
code from performance-improving changes and the use of virtualisation allows
otherwise obsolete operating environments to be maintained. In all the long
history and particular demands of mainframe computing lends the model a
robustness and reliability that supercomputers have yet to match and which for
certain problems is unlikely to be challenged.

\section{Mainframe Virtualization and Clustering}

Virtualization is, in essence, the ability to split a computing system into
multiple parts, called partitions. Each of these partitions can then run their
own operating system, called a Guest OS. These guests can then operate
independently of each other\cite{m4}. From the perspective of such an operating
system it has complete control of its underlying hardware, which is in fact
controlled by a program called a Hypervisor, such as VM on some IBM
mainframes\cite{m2}. The Hypervisor allows the administrator to create and delete
partitions, manage networking between the Guest OSs and dynamically reallocate
resources to the partitions. 

Clustering allows multiple systems to appear as one, providing redundancy in
the event that one should fail and increased resources. For IBM mainframes this
is done using Parallel Sysplex\cite{m5} which allows up to 32 systems to be linked
together. Each Guest OS would then runs in a Logical Partition(LAPR) hosted by a
Parallel Sysplex. In combination with GDPS\cite{m6}, which allows the linked systems
to be spread across many sites and provide continuous service, even in the case
of the complete failure of a site. In combination with the general reliability
of a mainframe and its ability to be repaired while still running, this greatly
reduces the chances of downtime and thus ensures high availability.

The non-mainframe world of x86 has also seen attempts at virtualization, these
include VMware and Xen. This is made difficult however by the fact that
the x86 instruction set was not designed with virtualization in mind\cite{m7}.
Solutions such as paravirtulization and new chip technologies such as
Pacifia\cite{m8} from AMD. These technologies are not advanced to the same extent as
those on the mainframe, which have been in use since the 1970s\cite{m3}.

In the area of web services, virtualization general advantages show in allowing
a number of existing services spread across a number of small-scale system to
be all run on a mainframe within a Guest OS. This removes the need for
maintenance for all the networking equipment required to hook up each individual
system, as now only the network connections to the mainframe itself need
maintenance. The ability to reallocate resources on the fly allow services which
receive more load than they can handle temporarily to get more resources,
without the need for a hardware upgrade, by using spare capacity or
deallocating resources from other services that are under utilised.

\section{Alternatives to the Mainframe}

Super computers, such as grids and lattices have proven to be a
competitor to the mainframe. The fundamental difference a super computer
exhibits is that it consists of many inexpensive computers connected
together via a network. Depending on the design, some super computers 
are able to be repaired while in operation, just like a mainframe. 
Alternatively, corporations such as Google and Amazon simply add and 
subtract nodes and groups of nodes to their clusters in order to 
carry out repairs, to make upgrades or replacements without adversely 
affecting the system as a whole.

Many vendors and manufacturers use the term Total Cost Ownership (TCO)
to try and compare their products against mainframe alternatives. Due to
the much higher hardware costs of a mainframe, they try to argue that
their products are cheaper to run due to lower hardware costs. However,
true TCO comparisons between IT infrastructure also involve software
and administrative personnel costs. Many TCO comparisons are
commissioned and paid for by software or hardware manufacturers, thus
skewing the results in their favour. The general consensus is that the
higher cost of a mainframe can sometimes be offset against lower
software and personnel costs, depending on how the mainframe is being
used.

\section{The Fall and Resurrection of the Mainframe}

Traditionally, Mainframes were accessed using monochrome, text 
based "Dumb Terminals" such as the DEC VT100 over DB9 serial 
connections. In the 1980s and 1990s, dumb terminals were being replaced
by personal computers (PCs) on office desks when networking PCs together
became popular. As a result, mainframes were often referred to as
"dinosaurs" and their days were believed to be numbered. These
speculations were incorrect.

In the late 1990s, with the popularity of web services growing, 
companies found new uses for their mainframes. By using server
virtualisation, they could offer web server performance similar 
to that of hundreds of smaller machines. The advantages of this
approach included lower power consumption and lower administrative 
costs. \cite[The growth of e-business has also dramatically increased the 
number of backend transactions processed by tried-and-true mainframe 
software as well as the size and throughput of databases. In late 2004,
IBM's mainframe revenues were increasing even with price reductions, 
thanks to attractive TCOs.]{m1}

\section{Conclusion -- The Mainframe over the Next 10 Years}
Mainframes are in many ways deficient compared to modern alternatives. They are
costly to set up, monolithic to maintain and generally uneconomical to upgrade.
However just as massively parallel, networked and grid computing have
particular problem areas to which they are ideally suited, mainframe computing
too has a number of applications which are perfectly supported by the platform.
The vital importance of tasks requiring little computational ability but
enormous data-handling capacity mean that single, unified system architectures
will have at least a key infrastructural role for the foreseeable future. As
noted by Arcati in \cite{m9}, ``Not only did the mainframe not die, but it has
reinvented itself and is now set to dominate the market for the next decade''.


\section{Bibliography}

\bibliography{Mainframes}

\end{document}
